import streamlit as st
import torch
import torch.nn as nn
import pickle
import re
import os

class ChessEloBiLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, dropout=0.3):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=2)
        self.dropout = nn.Dropout(dropout)
        self.fc1 = nn.Linear(hidden_dim * 2, 128)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 1)

    def forward(self, x):
        x = self.embedding(x)
        _, (h, _) = self.lstm(x)
        h = torch.cat((h[-2], h[-1]), dim=1)  # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä—è–º–æ–µ –∏ –æ–±—Ä–∞—Ç–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        h = self.dropout(h)
        h = self.relu(self.fc1(h))
        out = self.fc2(h)
        return out.squeeze(1)


@st.cache_resource
def load_model_and_encoder():
    base_dir = os.path.dirname(os.path.abspath(__file__))
    dataset_path = os.path.join(base_dir, "../dataset")
    model_path = os.path.join(base_dir, "../NeuronNet/elo_bilstm_model.pth")

    # –∑–∞–≥—Ä—É–∑–∫–∞ —ç–Ω–∫–æ–¥–µ—Ä–∞ —Ö–æ–¥–æ–≤
    with open(os.path.join(dataset_path, "move_encoder.pkl"), "rb") as f:
        encoder = pickle.load(f)

    # –∑–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    with open(os.path.join(dataset_path, "rating_norm.pkl"), "rb") as f:
        norm = pickle.load(f)

    vocab_size = len(encoder.classes_)
    model = ChessEloBiLSTM(vocab_size=vocab_size)
    model.load_state_dict(torch.load(model_path, map_location=torch.device("cpu")))
    model.eval()

    return model, encoder, norm


model, encoder, norm = load_model_and_encoder()


def clean_and_encode_moves(text, encoder, max_moves=60):
    text = re.sub(r"\d+\.", "", text)
    text = re.sub(r"1-0|0-1|1/2-1/2|\*", "", text)
    text = text.strip()
    moves = text.split()
    moves = moves[:max_moves]
    encoded = []
    for m in moves:
        if m in encoder.classes_:
            encoded.append(encoder.transform([m])[0])
        else:
            encoded.append(0)
    # padding
    while len(encoded) < max_moves:
        encoded.append(0)
    return torch.tensor([encoded], dtype=torch.long)


st.title("Chess ELO Predictor")
st.write("–í–≤–µ–¥–∏—Ç–µ –ø–∞—Ä—Ç–∏—é –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —à–∞—Ö–º–∞—Ç–Ω–æ–π –Ω–æ—Ç–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, `e4 e5 Nf3 Nc6 Bb5 a6 ...`)")

user_input = st.text_area("–•–æ–¥—ã:", height=150)

if st.button("üîÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ä–µ–π—Ç–∏–Ω–≥"):
    if len(user_input.strip()) == 0:
        st.warning("–í–≤–µ–¥–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ö–æ–¥.")
    else:
        encoded_moves = clean_and_encode_moves(user_input, encoder)
        with torch.no_grad():
            pred_norm = model(encoded_moves).item()

        # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        prediction = pred_norm * norm["std"] + norm["mean"]

        st.success(f"–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –∏–≥—Ä–æ–∫–∞: **{prediction:.0f} ELO**")
